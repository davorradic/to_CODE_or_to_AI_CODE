---
sidebar_position: 3
---

# Research Methodology

## üî¨ Research Design

Our study employs a **mixed-methods comparative design** that combines quantitative performance metrics with qualitative insights from participant experiences.

### Core Approach
- **Controlled Comparison**: AI-assisted teams vs. Traditional development teams
- **Multi-level Analysis**: Different experience groups tackle identical challenges
- **Real-world Context**: Actual company problems, not academic exercises
- **Longitudinal Tracking**: Pre-, during, and post-hackathon assessments

## üë• Participant Stratification

### Six Distinct Groups
We've designed specific participant categories to capture the full spectrum of programming experience and AI readiness:

1. **üå± Blank Slate Coders** - Zero programming experience
2. **üîç Code Curious** - Theoretical knowledge, minimal practice
3. **üõ†Ô∏è Traditional Builders** - 1-3 years traditional coding
4. **ü§ñ AI Natives** - AI-first development experience
5. **üéì Hybrid Learners** - Mixed traditional/AI experience
6. **üè¢ Industry Veterans** - 5+ years professional experience

### Strategic Distribution
- **Equal team sizes** across AI-assisted vs. traditional approaches
- **Controlled variables** such as problem complexity and team composition
- **Diverse backgrounds** to ensure representative findings

## üìã Data Collection Methods

### Quantitative Metrics
- **Performance Indicators**: Time to completion, code quality, feature completeness
- **Knowledge Assessments**: Pre/post fundamental concepts tests (20 questions each)
- **Collaboration Effectiveness**: Frequency and quality of AI interactions
- **Adaptation Speed**: Time to recover from requirement changes

### Qualitative Insights
- **Real-time Observations**: Embedded researchers document decision-making processes
- **Daily Reflections**: Structured interviews about challenges and breakthroughs
- **Critical Incident Analysis**: Deep dives into moments of confusion or clarity
- **TEDx Presentations**: Participants articulate their own discoveries

## üéØ Experimental Design

### Phase 1: Assessment (Day 1 Morning)
- **Baseline Knowledge Test**: 20 fundamental programming concepts
- **AI Tool Familiarity Survey**: Current experience with AI development tools
- **Problem-Solving Approach Interview**: How participants typically tackle new challenges

### Phase 2: Development Challenge (Days 1-3)
- **Identical Problems**: All teams work on the same company-provided challenges
- **Tool Restrictions**: Clear separation between AI-assisted and traditional teams
- **Progress Tracking**: Regular check-ins and milestone assessments
- **Pivotal Moment**: Mid-hackathon requirement changes test adaptability

### Phase 3: Knowledge Sharing (Day 4)
- **TEDx-Style Presentations**: Participants explain their discoveries
- **Cross-Group Dialogue**: Structured discussions between different participant types
- **Reflection Synthesis**: Collaborative mapping of essential vs. delegatable skills

## üìä Analysis Framework

### Border Identification Process
1. **Skill Categorization**: Map all observed programming activities
2. **Success Correlation**: Identify which skills predict AI collaboration success
3. **Failure Analysis**: Understand where AI assistance breaks down
4. **Cross-Group Comparison**: Find patterns across experience levels

### Expected Border Categories

#### ‚úÖ Above the Border (Essential Human Skills)
- **Conceptual Understanding**: What variables, functions, and logic mean
- **Problem Decomposition**: Breaking complex challenges into manageable pieces
- **AI Collaboration**: Writing effective prompts and evaluating suggestions
- **Quality Assessment**: Recognizing correct vs. incorrect solutions

#### ‚ùå Below the Border (AI-Delegatable Tasks)
- **Syntax Memorization**: Language-specific formatting and rules
- **Documentation Lookup**: Finding function parameters and library usage
- **Boilerplate Generation**: Repetitive code patterns and templates
- **Basic Debugging**: Identifying common syntax and logic errors

### Validation Methods
- **Inter-rater Reliability**: Multiple researchers analyze the same data
- **Participant Validation**: Subjects review and confirm findings
- **Industry Review**: Professional developers evaluate practical relevance
- **Academic Peer Review**: Methodology scrutinized by education researchers

## üåç Open Science Approach

### Transparency Principles
- **Open Methodology**: Complete research protocol available pre-study
- **Raw Data Access**: Anonymized datasets published for verification
- **Replication Package**: Full materials for study reproduction
- **Living Documentation**: Real-time updates during data collection

### Global Collaboration
- **International Advisory Board**: Educators from multiple countries
- **Multi-institutional Validation**: Results tested across different educational contexts
- **Crowd-sourced Verification**: Open challenges for methodology improvement

This methodology ensures our findings will be robust, applicable, and valuable for educational institutions worldwide.